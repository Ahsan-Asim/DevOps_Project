Part 1:

I have done h first part successfully an ddelieverables are listed:


DevOps_Project/
├── CMS_Backend/
│   ├── Dockerfile         # Dockerfile for the backend
│   ├── index.js           # Main server file
│   ├── connection.js      # MongoDB connection logic
│   ├── package.json       # Backend package.json
│   ├── model/
│   │   └── User.js        # User model
│   ├── routes/
│   │   ├── schoolRoutes.js # School routes
│   │   └── userRoutes.js   # User routes
│   └── .env               # Environment variables
└── CMS_Frontend/
    ├── Dockerfile         # Dockerfile for the frontend
    ├── package.json       # Frontend package.json
    ├── src/
    │   ├── App.js         # Main App component
    │   ├── components/     # React components
    │   ├── styles/        # CSS or styling files
    │   └── index.js       # Entry point for the React app
    └── public/            # Public assets


Dockerfile for frontend:
# Stage 1: Build
FROM node:alpine AS build
WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm install

# Copy application code
COPY . .

# Build the application
RUN npm run build

# Stage 2: Serve the application
FROM nginx:alpine
COPY --from=build /app/build /usr/share/nginx/html

# Copy custom Nginx configuration (if any)
# COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port
EXPOSE 80

# Start Nginx
CMD ["nginx", "-g", "daemon off;"]



Explanation for optimization:

from above Dockerfile of frontend:
The frontend Dockerfile utilizes a multi-stage build process to enhance optimization and efficiency. By separating the build environment from the production runtime, it ensures that the final image contains only the essential components, minimizing its size. The use of lightweight base images like node:alpine and nginx:alpine significantly reduces the overall footprint, facilitating faster deployment and lower resource consumption.

Additionally, layer caching optimizes build times by allowing Docker to cache the layers for unchanged dependencies, speeding up subsequent builds. The final image comprises only the built application artifacts and the Nginx server, excluding unnecessary development dependencies, which enhances security by reducing the attack surface. This streamlined structure not only improves maintainability and readability but also ensures efficient resource management in production scenarios.



Dockerfile for backend:
# Stage 1: Build
FROM node:alpine AS build
WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm install

# Copy application code
COPY . .

# Build the application (if you have a build step)
# RUN npm run build

# Stage 2: Production
FROM node:alpine

WORKDIR /app

# Copy only the necessary files from the build stage
COPY --from=build /app ./

# Install only production dependencies
RUN npm install --only=production

# Create a non-root user for security
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
USER appuser

# Expose port
EXPOSE 3001

# Start the application
CMD ["node", "index.js"]  # Adjust to your server entry point







Explanation for optimization:
The backend Dockerfile uses a multi-stage build to create a lean production image. It starts with a lightweight Node.js image to install dependencies and copy the application code. In the production stage, it uses another lightweight Node.js image, copying only the necessary files from the build stage. By installing only production dependencies, it keeps the image small and secure.

To enhance security, it creates a non-root user to run the application, minimizing risks. Finally, it exposes the required port and starts the application. This approach results in an efficient and secure production environment for the backend.


frontend is running:

Screenshot1





Now I am done with my 2nd part:
docker-compose.yml file with complete things like ith proper setup for each service, including bind mounts and configurations for hot-reloading.:


version: '3.8'

services:
  mongodb:
    image: mongo
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db

  cms-backend:
    build:
      context: ./CMS_Backend/server  # Ensure this points to the correct backend directory
    container_name: cms-backend
    ports:
      - "3001:3001"  # Map backend port
    volumes:
      - ./CMS_Backend/server:/app  # Bind mount for hot reloading
      - /app/node_modules  # Prevent node_modules from being overwritten
    environment:
      - NODE_ENV=development
    command: nodemon index.js  # Use nodemon for hot reloading

  cms-frontend:
    build:
      context: ./CMS_Frontend/frontend  # Ensure this points to the correct frontend directory
    container_name: cms-frontend
    ports:
      - "3000:80"  # Map frontend port to Nginx
    volumes:
      - ./CMS_Frontend/frontend/build:/app/build  # Mount the build folder for Nginx
      - ./CMS_Frontend/frontend:/app  # Bind mount for hot reloading
    depends_on:
      - cms-backend  # Ensure backend starts before frontend
    command: npm start  # Start the frontend

volumes:
  mongo-data:


Screenshot2
screenshot 3





Part 3:

I am done with kubernetes clster working by sing kstomization:


steps:
1. setup and installed kubernetes and minikube
2. install and setup kustomize


3. inside main folder of DevOps_Project create 2 folders 1. base 2. overlay
inside base:
deployment.yaml
service.yaml
kustomization.yaml


so basically 3 deployment files, 3 service files and 1 kustomization file and you can see code as files are attache in assignment folder as names:
  - mongodb-deployment.yaml
  - mongodb-service.yaml
  - backend-deployment.yaml
  - backend-service.yaml
  - frontend-deployment.yaml
  - frontend-service.yaml
  -kustomization.yml



and inside overlay another directory production and inside production:
kustomization.yml fiel that conatins for prodyction


4. then start minikube:
screenshot5



5. then inside production run command for starting things:
it takes too much time and give so many errors but after somtime:
screenshot 6


as you can see in screenshot that all services are rnning successfully



7. then try services lke for test start frontend so it atomatically open it in browser:

screenshot 7



Now RBCA working:

steps:

1. Create a service account 
you can see attached file named cms-service-account.yaml

then apply mainfiest

2. create role.yaml fle as yo can see attached
then apply role

3. cms-rolebinding.yaml created attached

apply binding

4. test-pod.yaml for testing creation

after testing all deleted


screenshot 8



now Set up service discovery using Ingress or service mesh (Istio/Linkerd) for advanced use cases.

steps:

1. Install an Ingress Controller


2. create cms-ingress.yaml as attached

then run file


3. add ip with test link to the sudo notepad file

screenshot10


4. now run by using this http://cms.example.com

screesht 11





Now Part 4:
steps:

    Create Namespace: A dedicated namespace for MongoDB was created.
        File: mongodb-namespace.yaml

    Define Persistent Volume (PV): A PV was created to provide storage for MongoDB.
        File: mongo-pv.yaml

    Define Persistent Volume Claim (PVC): A PVC was created to claim the storage defined in the PV.
        File: mongo-pvc.yaml

    Create StatefulSet: MongoDB was deployed as a StatefulSet to manage stateful applications.
        File: mongo-statefulset.yaml

    Deploy Resources: The configuration files were applied to the Kubernetes cluster.
        Command: kubectl apply -f mongodb-namespace.yaml, kubectl apply -f mongo-pv.yaml, kubectl apply -f mongo-pvc.yaml, kubectl apply -f mongo-statefulset.yaml

Verification Steps

    Check the deployment of StatefulSets and Pods.
    Verify that the PVC is bound correctly.
    Access the MongoDB shell to confirm functionality and data persistence.
    
    
    
 screenshot12
 
 screenshot13
 Screensht14
 
 
 
 Part 5:
 
 Service Discovery and Networking using ingress
 
 steps:
 
 NGINX Ingress Controller Installation: I deployed the NGINX Ingress Controller, which manages external access to services within the cluster.

    File Name: ingress-nginx-manifest.yaml

Ingress Resource Creation: I created an Ingress resource to define routing rules for external traffic directed to the frontend service. This included specifying the host and the corresponding service.

    File Name: cms-ingress.yaml

Ingress Resource Application: I applied the Ingress resource to the cluster to activate the defined routing rules.

Ingress Configuration Verification: I verified the Ingress setup by checking the status and configuration of the Ingress resources.

Frontend Access: After configuring the Ingress, I ensured that external traffic could reach the frontend service through the defined routes.

files are attached.

screenshot 15
16
17
18
19



Part 6:

RBAC has already done in part 3 and below files are attached for this task:

cms-service-account.yaml
cms-role.yaml
cms-role-binding.yaml



Now for the others:

Network Policies:

    Define network policies to control traffic between pods:
        Filename: cms-network-policy.yaml

Secrets Management:

    Create a secret to store sensitive information (e.g., database passwords, API keys):
        Filename: cms-secrets.yaml

Apply Configurations:

    Use kubectl apply to deploy the above configurations in the Kubernetes cluster.

Verification:

    Check the applied configurations by listing service accounts, roles, role bindings, and secrets:
        Commands: kubectl get serviceaccounts, kubectl get roles, kubectl get rolebindings, and kubectl get secrets.



